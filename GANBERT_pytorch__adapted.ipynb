{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GANBERT_pytorch _adapted.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpqAwtN8rTA"
      },
      "source": [
        "# GAN-BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFGGmY1XVpPg"
      },
      "source": [
        "This is adapted from the open source implementation available at https://github.com/crux82/ganbert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqpm34x2rms",
        "outputId": "3ac263a9-a433-4de5-e625-5c1e2d05b0e2"
      },
      "source": [
        "!pip install transformers==4.3.2\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install sentencepiece\n",
        "\n",
        "##Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.3.2 in /usr/local/lib/python3.7/dist-packages (4.3.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.2) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZgRup520II",
        "outputId": "c71b0579-41b8-4de9-c431-bb6a540c5827"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3ns8Ic7I-h"
      },
      "source": [
        "### Input Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw0HC_hU3FUy",
        "outputId": "376c6046-7bbf-4c2d-b171-32cbf7cf4551"
      },
      "source": [
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 64\n",
        "batch_size = 64\n",
        "\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator, \n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1; \n",
        "# number of hidden layers in the discriminator, \n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1; \n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.2\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets, \n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8\n",
        "num_train_epochs = 10\n",
        "multi_gpu = True\n",
        "# Scheduler\n",
        "apply_scheduler = False\n",
        "warmup_proportion = 0.1\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
        "# (or add) transformer models compatible with GAN\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "#model_name = \"bert-base-uncased\"\n",
        "#model_name = \"roberta-base\"\n",
        "#model_name = \"albert-base-v2\"\n",
        "#model_name = \"xlm-roberta-base\"\n",
        "#model_name = \"amazon/bort\"\n",
        "\n",
        "#--------------------------------\n",
        "#  Retrieve the TREC QC Dataset\n",
        "#--------------------------------\n",
        "! git clone https://github.com/crux82/ganbert\n",
        "\n",
        "#  NOTE: in this setting 50 classes are involved\n",
        "labeled_file = \"./ganbert/data/labeled.tsv\"\n",
        "unlabeled_file = \"./ganbert/data/unlabeled.tsv\"\n",
        "test_filename = \"./ganbert/data/test.tsv\"\n",
        "\n",
        "label_list = [\"UNK_UNK\",\"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \n",
        "              \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \n",
        "              \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \n",
        "              \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \n",
        "              \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \n",
        "              \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \n",
        "              \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \n",
        "              \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \n",
        "              \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \n",
        "              \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \n",
        "              \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \n",
        "              \"NUM_weight\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ganbert' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Q5jzVioTHb"
      },
      "source": [
        "Load the Tranformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxghkkZq3Gbn"
      },
      "source": [
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_ixn5qn_zV"
      },
      "source": [
        "Function required to load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7cP8q7K3BId"
      },
      "source": [
        "def get_qc_examples(input_file):\n",
        "  \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "  examples = []\n",
        "\n",
        "  with open(input_file, 'r') as f:\n",
        "      contents = f.read()\n",
        "      file_as_list = contents.splitlines()\n",
        "      for line in file_as_list[1:]:\n",
        "          split = line.split(\" \")\n",
        "          question = ' '.join(split[1:])\n",
        "\n",
        "          text_a = question\n",
        "          inn_split = split[0].split(\":\")\n",
        "          label = inn_split[0] + \"_\" + inn_split[1]\n",
        "          examples.append((text_a, label))\n",
        "      f.close()\n",
        "\n",
        "  return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K43tOavNqib4"
      },
      "source": [
        "**Load** the input QC dataset (fine-grained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXCwFyF2qhw7"
      },
      "source": [
        "#Load the examples\n",
        "labeled_examples = get_qc_examples(labeled_file)\n",
        "unlabeled_examples = get_qc_examples(unlabeled_file)\n",
        "test_examples = get_qc_examples(test_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhaW5vBfR6B"
      },
      "source": [
        "Functions required to convert examples into Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmKL5AD7I4Zg"
      },
      "source": [
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
        "  '''\n",
        "  Generate a Dataloader given the input examples, eventually masked if they are \n",
        "  to be considered NOT labeled.\n",
        "  '''\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples  \n",
        "  num_labeled_examples = 0\n",
        "  for label_mask in label_masks:\n",
        "    if label_mask: \n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(input_examples): \n",
        "    if label_mask_rate == 1 or not balance_label_examples:\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "  \n",
        "  #-----------------------------------------------\n",
        "  # Generate input examples to the Transformer\n",
        "  #-----------------------------------------------\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # Tokenization \n",
        "  for (text, label_mask) in examples:\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "    label_id_array.append(label_map[text[1]])\n",
        "    label_mask_array.append(label_mask)\n",
        "  \n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "    input_mask_array.append(att_mask)\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids) \n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = sampler(dataset), \n",
        "              batch_size = batch_size) # Trains with this batch size.\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3O-VeefT3g"
      },
      "source": [
        "Convert the input examples into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c-nsMXlKX-D"
      },
      "source": [
        "label_map = {}\n",
        "for (i, label) in enumerate(label_list):\n",
        "  label_map[label] = i\n",
        "#------------------------------\n",
        "#   Load the train dataset\n",
        "#------------------------------\n",
        "train_examples = labeled_examples\n",
        "#The labeled (train) dataset is assigned with a mask set to True\n",
        "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "#If unlabel examples are available\n",
        "if unlabeled_examples:\n",
        "  train_examples = train_examples + unlabeled_examples\n",
        "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
        "\n",
        "#------------------------------\n",
        "#   Load the test dataset\n",
        "#------------------------------\n",
        "#The labeled (test) dataset is assigned with a mask set to True\n",
        "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "\n",
        "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18kY64-n3I6y"
      },
      "source": [
        "#------------------------------\n",
        "#   The Generator as in \n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep\n",
        "\n",
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylz5rvqE3U2S",
        "outputId": "66308472-8816-443a-e318-3c3b0f4dfd3b"
      },
      "source": [
        "# The config file is required to get the dimension of the vector produced by \n",
        "# the underlying transformer\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "# Define the number and width of hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "#-------------------------------------------------\n",
        "#   Instantiate the Generator and Discriminator\n",
        "#-------------------------------------------------\n",
        "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
        "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
        "\n",
        "# Put everything in the GPU if available\n",
        "if torch.cuda.is_available():    \n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "print(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhqylHGK3Va4",
        "outputId": "c827742d-b100-4f1c-bad9-4d1b1810a095"
      },
      "source": [
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "#models parameters\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "#optimizer\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
        "\n",
        "#scheduler\n",
        "if apply_scheduler:\n",
        "  num_train_examples = len(train_examples)\n",
        "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    transformer.train() \n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every print_each_n_step batches.\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "     \n",
        "        # Encode real data in the Transformer\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        \n",
        "        # Generate fake data that should have the same distribution of the ones\n",
        "        # encoded by the transformer. \n",
        "        # First noisy input are used in input to the Generator\n",
        "        noise = torch.zeros(b_input_ids.shape[0],noise_size, device=device).uniform_(0, 1)\n",
        "        # Gnerate Fake data\n",
        "        gen_rep = generator(noise)\n",
        "\n",
        "        # Generate the output of the Discriminator for real and fake data.\n",
        "        # First, we put together the output of the tranformer and the generator\n",
        "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "        # Then, we select the output of the disciminator\n",
        "        features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "        # Finally, we separate the discriminator's output for the real and fake\n",
        "        # data\n",
        "        features_list = torch.split(features, batch_size)\n",
        "        D_real_features = features_list[0]\n",
        "        D_fake_features = features_list[1]\n",
        "      \n",
        "        logits_list = torch.split(logits, batch_size)\n",
        "        D_real_logits = logits_list[0]\n",
        "        D_fake_logits = logits_list[1]\n",
        "        \n",
        "        probs_list = torch.split(probs, batch_size)\n",
        "        D_real_probs = probs_list[0]\n",
        "        D_fake_probs = probs_list[1]\n",
        "\n",
        "        #---------------------------------\n",
        "        #  LOSS evaluation\n",
        "        #---------------------------------\n",
        "        # Generator's LOSS estimation\n",
        "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "  \n",
        "        # Disciminator's LOSS estimation\n",
        "        logits = D_real_logits[:,0:-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        # The discriminator provides an output for labeled and unlabeled real data\n",
        "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
        "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "        # It may be the case that a batch does not contain labeled examples, \n",
        "        # so the \"supervised loss\" in this case is not evaluated\n",
        "        if labeled_example_count == 0:\n",
        "          D_L_Supervised = 0\n",
        "        else:\n",
        "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "                 \n",
        "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        #---------------------------------\n",
        "        #  OPTIMIZATION\n",
        "        #---------------------------------\n",
        "        # Avoid gradient accumulation\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "\n",
        "        # Calculate weigth updates\n",
        "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward() \n",
        "        \n",
        "        # Apply modifications\n",
        "        gen_optimizer.step()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "        # A detail log of the individual losses\n",
        "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
        "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
        "        #             g_loss_d, g_feat_reg))\n",
        "\n",
        "        # Save the losses to print them later\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "        # Update the learning rate with the scheduler\n",
        "        if apply_scheduler:\n",
        "          scheduler_d.step()\n",
        "          scheduler_g.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss generator: {0:.3f}\".format(avg_train_loss_g))\n",
        "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #     TEST ON THE EVALUATION DATASET\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our test set.\n",
        "    print(\"\")\n",
        "    print(\"Running Test...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    transformer.eval() #maybe redundant\n",
        "    discriminator.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_test_accuracy = 0\n",
        "   \n",
        "    total_test_loss = 0\n",
        "    nb_test_steps = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels_ids = []\n",
        "\n",
        "    #loss\n",
        "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "            hidden_states = model_outputs[-1]\n",
        "            _, logits, probs = discriminator(hidden_states)\n",
        "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "            filtered_logits = logits[:,0:-1]\n",
        "            # Accumulate the test loss.\n",
        "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "            \n",
        "        # Accumulate the predictions and the input labels\n",
        "        _, preds = torch.max(filtered_logits, 1)\n",
        "        all_preds += preds.detach().cpu()\n",
        "        all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    all_preds = torch.stack(all_preds).numpy()\n",
        "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    avg_test_loss = avg_test_loss.item()\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    test_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "    print(\"  Test took: {:}\".format(test_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss generator': avg_train_loss_g,\n",
        "            'Training Loss discriminator': avg_train_loss_d,\n",
        "            'Valid. Loss': avg_test_loss,\n",
        "            'Valid. Accur.': test_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Test Time': test_time\n",
        "        }\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:29.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:47.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:05.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:23.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:41.\n",
            "\n",
            "  Average training loss generator: 0.739\n",
            "  Average training loss discriminator: 2.845\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.464\n",
            "  Test Loss: 2.704\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.721\n",
            "  Average training loss discriminator: 1.455\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.452\n",
            "  Test Loss: 2.617\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.728\n",
            "  Average training loss discriminator: 1.013\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.556\n",
            "  Test Loss: 2.512\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.716\n",
            "  Average training loss discriminator: 0.888\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.602\n",
            "  Test Loss: 2.471\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.714\n",
            "  Average training loss discriminator: 0.808\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.598\n",
            "  Test Loss: 2.610\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.712\n",
            "  Average training loss discriminator: 0.777\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.592\n",
            "  Test Loss: 2.654\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.713\n",
            "  Average training loss discriminator: 0.759\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.602\n",
            "  Test Loss: 2.750\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.712\n",
            "  Average training loss discriminator: 0.751\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.608\n",
            "  Test Loss: 2.775\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.710\n",
            "  Average training loss discriminator: 0.740\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.604\n",
            "  Test Loss: 2.845\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:18.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:36.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:54.\n",
            "  Batch    40  of     92.    Elapsed: 0:01:12.\n",
            "  Batch    50  of     92.    Elapsed: 0:01:30.\n",
            "  Batch    60  of     92.    Elapsed: 0:01:48.\n",
            "  Batch    70  of     92.    Elapsed: 0:02:06.\n",
            "  Batch    80  of     92.    Elapsed: 0:02:24.\n",
            "  Batch    90  of     92.    Elapsed: 0:02:42.\n",
            "\n",
            "  Average training loss generator: 0.710\n",
            "  Average training loss discriminator: 0.734\n",
            "  Training epoch took: 0:02:45\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.608\n",
            "  Test Loss: 2.890\n",
            "  Test took: 0:00:03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDm9NProRB4c",
        "outputId": "f4cd3776-f9d8-4d76-83e4-b77a0aa5ec7a"
      },
      "source": [
        "for stat in training_stats:\n",
        "  print(stat)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'Training Loss generator': 0.7390733490819517, 'Training Loss discriminator': 2.844949351704639, 'Valid. Loss': 2.7041358947753906, 'Valid. Accur.': 0.464, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 2, 'Training Loss generator': 0.7210313049347504, 'Training Loss discriminator': 1.4546030267425205, 'Valid. Loss': 2.6173174381256104, 'Valid. Accur.': 0.452, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 3, 'Training Loss generator': 0.7275488862524861, 'Training Loss discriminator': 1.0127850103637446, 'Valid. Loss': 2.5117886066436768, 'Valid. Accur.': 0.556, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 4, 'Training Loss generator': 0.7164584080810132, 'Training Loss discriminator': 0.888008572485136, 'Valid. Loss': 2.471228837966919, 'Valid. Accur.': 0.602, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 5, 'Training Loss generator': 0.7138760076916736, 'Training Loss discriminator': 0.8079386748697447, 'Valid. Loss': 2.610154151916504, 'Valid. Accur.': 0.598, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 6, 'Training Loss generator': 0.7119927386874738, 'Training Loss discriminator': 0.776621668882992, 'Valid. Loss': 2.6542210578918457, 'Valid. Accur.': 0.592, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 7, 'Training Loss generator': 0.7134455785803173, 'Training Loss discriminator': 0.759320172926654, 'Valid. Loss': 2.7499027252197266, 'Valid. Accur.': 0.602, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 8, 'Training Loss generator': 0.7119302270205125, 'Training Loss discriminator': 0.7505458159291226, 'Valid. Loss': 2.774604558944702, 'Valid. Accur.': 0.608, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 9, 'Training Loss generator': 0.709797326637351, 'Training Loss discriminator': 0.7404587968536045, 'Valid. Loss': 2.845186948776245, 'Valid. Accur.': 0.604, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "{'epoch': 10, 'Training Loss generator': 0.7102639072615168, 'Training Loss discriminator': 0.7336080806410831, 'Valid. Loss': 2.890346050262451, 'Valid. Accur.': 0.608, 'Training Time': '0:02:45', 'Test Time': '0:00:03'}\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:28:04 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfmvyACGsvYM",
        "outputId": "b8c98e16-bc69-4c1c-9286-1dac1a2cd6e8"
      },
      "source": [
        "# # from google.colab import files\n",
        "# # torch.save(generator.state_dict(), 'generator.pth')\n",
        "# # torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
        "# # # download checkpoint file\n",
        "# # files.download('generator.pth')\n",
        "# #For loading back\n",
        "# state_dict = torch.load('generator.pth')\n",
        "# generator.load_state_dict(state_dict)\n",
        "# state_dict = torch.load('discriminator.pth')\n",
        "# discriminator.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDk6HYIwtsPj"
      },
      "source": [
        "###Testing for fake data\n",
        "noice = torch.zeros(1,100,device = device).uniform_(0,1)\n",
        "gen_rep = generator(noice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3giwtvvZyj"
      },
      "source": [
        "gen_rep.shape\n",
        "features,logits,probs = discriminator(gen_rep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fidg84V66lhj"
      },
      "source": [
        "###Comparing with BASE Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWZYP6lv_kTP"
      },
      "source": [
        "\n",
        "class Base_Model(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Base_Model, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs\n",
        "base_model = Base_Model(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dOeGO9yr1EY",
        "outputId": "ab49d3fe-d21c-4760-f858-d62bcd3f3a60"
      },
      "source": [
        "\n",
        "base_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Base_Model(\n",
              "  (input_dropout): Dropout(p=0.2, inplace=False)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (logit): Linear(in_features=768, out_features=52, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpXSE5WKzeSH"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "  base_model.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMYDAZr0uKxQ",
        "outputId": "bb17579b-fe2b-4129-aaa4-0e995bc68c1f"
      },
      "source": [
        "training_stats_base = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "#models parameters\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "base_vars = transformer_vars + [v for v in base_model.parameters()]\n",
        "\n",
        "\n",
        "#optimizer\n",
        "base_optimizer = torch.optim.AdamW(base_vars, lr=learning_rate_discriminator)\n",
        "\n",
        "#scheduler\n",
        "if apply_scheduler:\n",
        "  num_train_examples = len(train_examples)\n",
        "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  scheduler_b = get_constant_schedule_with_warmup(base_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "  \n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    tr_b_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    transformer.train() \n",
        "    base_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every print_each_n_step batches.\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "     \n",
        "        # Encode real data in the Transformer\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        \n",
        "        features, logits, probs = base_model(hidden_states)\n",
        "\n",
        "\n",
        "        #---------------------------------\n",
        "        #  LOSS evaluation\n",
        "        #---------------------------------\n",
        "  \n",
        "        # Base Model's LOSS estimation\n",
        "        logits = logits[:,0:-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "        # It may be the case that a batch does not contain labeled examples, \n",
        "        # so the \"supervised loss\" in this case is not evaluated\n",
        "        if labeled_example_count == 0:\n",
        "          D_L_Supervised = 0\n",
        "        else:\n",
        "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "                 \n",
        "        base_loss = D_L_Supervised\n",
        "\n",
        "        #---------------------------------\n",
        "        #  OPTIMIZATION\n",
        "        #---------------------------------\n",
        "        # Avoid gradient accumulation\n",
        "        base_optimizer.zero_grad()\n",
        "\n",
        "        \n",
        "        # Apply modifications\n",
        "        base_optimizer.step()\n",
        "\n",
        "        # A detail log of the individual losses\n",
        "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
        "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
        "        #             g_loss_d, g_feat_reg))\n",
        "\n",
        "        # Save the losses to print them later\n",
        "\n",
        "        # Update the learning rate with the scheduler\n",
        "        if apply_scheduler:\n",
        "          scheduler_b.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss_b = tr_b_loss / len(train_dataloader)\n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss generator: {0:.3f}\".format(avg_train_loss_b))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #     TEST ON THE EVALUATION DATASET\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our test set.\n",
        "    print(\"\")\n",
        "    print(\"Running Test...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    transformer.eval() #maybe redundant\n",
        "    base_model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_test_accuracy = 0\n",
        "   \n",
        "    total_test_loss = 0\n",
        "    nb_test_steps = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels_ids = []\n",
        "\n",
        "    #loss\n",
        "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "            hidden_states = model_outputs[-1]\n",
        "            _, logits, probs = base_model(hidden_states)\n",
        "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "            filtered_logits = logits[:,0:-1]\n",
        "            # Accumulate the test loss.\n",
        "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "            \n",
        "        # Accumulate the predictions and the input labels\n",
        "        _, preds = torch.max(filtered_logits, 1)\n",
        "        all_preds += preds.detach().cpu()\n",
        "        all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    all_preds = torch.stack(all_preds).numpy()\n",
        "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    avg_test_loss = avg_test_loss.item()\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    test_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "    print(\"  Test took: {:}\".format(test_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats_base.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss base': avg_train_loss_b,\n",
        "            'Valid. Loss': avg_test_loss,\n",
        "            'Valid. Accur.': test_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Test Time': test_time\n",
        "        }\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:09.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:26.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:08.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:25.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:08.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:25.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:09.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:26.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:08.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:25.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:08.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:25.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:09.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:26.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:08.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:25.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:08.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:25.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of     92.    Elapsed: 0:00:04.\n",
            "  Batch    20  of     92.    Elapsed: 0:00:08.\n",
            "  Batch    30  of     92.    Elapsed: 0:00:13.\n",
            "  Batch    40  of     92.    Elapsed: 0:00:17.\n",
            "  Batch    50  of     92.    Elapsed: 0:00:21.\n",
            "  Batch    60  of     92.    Elapsed: 0:00:25.\n",
            "  Batch    70  of     92.    Elapsed: 0:00:30.\n",
            "  Batch    80  of     92.    Elapsed: 0:00:34.\n",
            "  Batch    90  of     92.    Elapsed: 0:00:38.\n",
            "\n",
            "  Average training loss generator: 0.000\n",
            "  Training epoch took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.020\n",
            "  Test Loss: 3.947\n",
            "  Test took: 0:00:03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZi35Cv_6fyz"
      },
      "source": [
        "Plotting fake data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "VGyUWKiUzSGw",
        "outputId": "4822546a-82b5-417c-8444-e98a4c092cbb"
      },
      "source": [
        "####Plotting the Fake Data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import statistics\n",
        "  \n",
        "# Plot between -10 and 10 with .001 steps.\n",
        "x_axis = np.arange(-20, 20, 0.01)\n",
        "  \n",
        "# Calculating mean and standard deviation\n",
        "mean = statistics.mean(x_axis)\n",
        "sd = statistics.stdev(x_axis)\n",
        "  \n",
        "plt.plot(x_axis, norm.pdf(x_axis, mean, sd))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVhWdf7/8eebXVBREFwABQRR3BVtGXUqtbRcstUWq6nJ9mbGNtu0fbJp+87UWE3LVFPZpqWmWVZmq4m7yCLiAriAIovscH9+f3CcH0OYoMC5l/fjuri8Ofe5b173ie4X9zmf8zlijEEppZTn8bI7gFJKKXtoASillIfSAlBKKQ+lBaCUUh5KC0AppTyUj90BmqNLly4mOjra7hhKKeVS1q1bd9AYE9ZwuUsVQHR0NMnJyXbHUEoplyIiuxtbrruAlFLKQ2kBKKWUh2pSAYjIBBFJF5FMEZndyP3+IvK+df8aEYm2lo8UkY3W1yYRmVbvMbtEZIt1n+7XUUqpNnbcYwAi4g28CIwHcoC1IrLYGLOt3mrXAYeNMXEiMh2YB1wKbAWSjDE1ItId2CQiS4wxNdbjzjTGHGzJF6SUUqppmvIJYCSQaYzJMsZUAQuAqQ3WmQq8ad3+CBgrImKMKav3Zh8A6MRDSinlJJpSABFAdr3vc6xlja5jveEXAaEAInKKiKQAW4Ab6xWCAb4QkXUiMvNYP1xEZopIsogk5+fnN+U1KaWUaoJWPwhsjFljjOkPjADuFZEA665RxphhwETgFhEZc4zHv2KMSTLGJIWF/WoYq1JKqRPUlPMAcoGoet9HWssaWydHRHyAYOBQ/RWMMakicgQYACQbY3Kt5Xkisoi6XU2rT+hVKNVGamod7DpURs7hMvJLKikoraLGYXA4DD7eXoQE+dI50I+eoYHEdAnC38fb7shKHVNTCmAtEC8iMdS90U8HLm+wzmLgauAn4CLga2OMsR6TbR0E7gX0BXaJSBDgZYwpsW6fDTzSMi9JqZZzpLKGNVmH+D7zIMm7DpNxoITKGkeTHuslEN0liFNiQjglJpTfxXUhrIN/KydWqumOWwDWm/etwArAG3jdGJMiIo9Q95f8YuA14G0RyQQKqCsJgFHAbBGpBhzAzcaYgyISCywSkaMZ3jXGfN7SL06pE1FRXcvXaXl8siGXVen5VNU68PfxYnivzlx1Wi/6dutIr9BAwjsEENLeDz9vL7wEqmodHC6rpuBIFTsPlZJ5oISUvcUs3byP937JxktgZEwI5w3qwZRBPQgO9LX7pSoPJ650RbCkpCSjU0Go1nKguIK3ftrFO2v2UFhWTVgHfyYP6sG4fuEM69WZAN8T251T6zBs21vMl6kHWLZlH5l5Rwjw9eL8IRFcdVo0iT06tuwLUaoBEVlnjEn61XItAOXp9haW838rt7NwQw41DsPZiV258tRenN67C95e0uI/L2VvEf/5eTeLNuRSUe1gXL9w/jyuDwMiglv8ZykFWgBK/UphWRX/XLWDf/+4CwxMHxnFdaNi6BUa1CY/v6ismrd+2sW/vsuiuKKGcwd2496J/YgKCWyTn688hxaAUhZjDJ9szOWxpakUlFVxwdBI/jI+nsjO9rzxFpVX8/r3O3lldRYOY7jlzDhmjok94V1OSjWkBaAUsPtQKfct2sIPmYcYEtWJJ6YNdJp98LmF5Tz+2TaWbdlPTJcgnrlkMMN6drY7lnIDWgDKoxlj+Hh9LnM/3YqXCHdP7MsVI3vi1Qr7+E/Wd9vzmf3xFvYVlXPzGXHcPjYePx+duFeduGMVgEtdEEapE1FUXs39i7awdPM+TokJ4blLh9CjUzu7Yx3T6Pgwlv95NI8s2cYL32TybUY+/7ximB4bUC1O/6xQbi3jQAlTXviez7fu565zEnj3+lOd+s3/qI4Bvjx98WBeunI4uw6VMukf3/NNep7dsZSb0QJQbuvzrfuZ9uIPlFXV8v4Np3LLmXGtMqyzNU0Y0I2lt42iR6d2XPvvtTy/MgNX2m2rnJsWgHI7xhj+b+V2bvzPOuK6dmDJraMY3ivE7lgnrFdoEAtvOp1pQyN4fuV2Zn2wicqaWrtjKTegxwCUW6mpdfDAJ1tZsDabC4dF8vi0AW4xnLKdnzfPXDyY3mHt+duKdHILy3llxnA6BfrZHU25MP0EoNxGeVUtN/5nHQvWZnPbWXE8ffEgt3jzP0pEuOXMOP5v+hA27inkgvk/srew3O5YyoVpASi3UFRezZWvreGrtDwendqfO85OwJps0O1MHRLB29eNJL+4kkte/ok9h8rsjqRclBaAcnlFZdXMeG0Nm3MKefHyYcw4LdruSK3ulNhQ3r3+VI5U1nDxyz+SmXfE7kjKBWkBKJdWVFb3l3/avhJeunI45w7sbnekNjMwMpj3Z55GrQMuffkn0vYX2x1JuRgtAOWyCsuquOK1n0nfX8JLM4Yxtl9XuyO1uYRuHfjghlPx9fbiylfXkJWvnwRU02kBKJd0pLKGq1//hYwDR3j5quGc1dfz3vyPig1rz3/+eArGwBWvriG7QI8JqKbRAlAup7KmlhveTmbr3mL+efkwzkwItzuS7eLC60qgrKqWK15dw/6iCrsjKRegBaBcSq3D8Jf3N/JD5iGeunAQ4xI99y//hvp178hb146koLSKK19bQ2FZld2RlJPTAlAuwxjDA59sZdmW/TxwXj8uHB5pdySnMziqE69encSeQ2XMfGsdFdV6xrA6Ni0A5TL+8XUm7/2yh5vP6M0fR8faHcdpnRobytOXDOaXXQXc8eEmHA6dO0g1TqeCUC5h8aa9PPtlBhcMjeCucxLsjuP0pgzuwb7Ccv66PI2ITu2479x+dkdSTkgLQDm9dbsPc+eHmxgZHcJfLxzotmf4trSZY2Lr5gxanUVk53Zc5QEnyKnm0QJQTi27oIyZbyXTPTiAl2YMx9/Hfeb2aW0iwtzJ/ck9XM7DS7YRF96e03t3sTuWciJ6DEA5rZKKaq7991qqax28fs0IQoJ05svm8vYSnp8+hJguQdzyzno9R0D9Dy0A5ZSMMdz54SayDpYy/8rh9A5rb3ckl9UhwJd/XZVErcNw/VvJlFbW2B1JOQktAOWU5n+7gxUpB7h3Yl9+F6e7LU5WTJcgXrh8GBkHSpj1wUYdGaQALQDlhFZn5PP0inQmDerOdaNi7I7jNsb0CeO+c/uxIuUA/1yVaXcc5QS0AJRTyS4o4/YFG4gP78BTFw3SET8t7LpRMUwZ3INnv8zgxx0H7Y6jbNakAhCRCSKSLiKZIjK7kfv9ReR96/41IhJtLR8pIhutr00iMq2pz6k8T0V1LTe9s47aWsNLM4YT6KeD1FqaiPDXCwYS0yWI29/bSF6xzhnkyY5bACLiDbwITAQSgctEJLHBatcBh40xccBzwDxr+VYgyRgzBJgAvCwiPk18TuVhHl26ja25xTx7ad2oFdU6gvx9mH/lcEora7j1vQ3U1DrsjqRs0pRPACOBTGNMljGmClgATG2wzlTgTev2R8BYERFjTJkx5uiQgwDg6JGnpjyn8iDLtuzjnTV7mDkmlvE6wVur69O1A49PG8AvOwt45ssMu+MomzSlACKA7Hrf51jLGl3HesMvAkIBROQUEUkBtgA3Wvc35TmxHj9TRJJFJDk/P78JcZWryS4o456PNzM4qhN3nq3TPLSVC4ZFctnIKOav2sHXaQfsjqNs0OoHgY0xa4wx/YERwL0iEtDMx79ijEkyxiSFhYW1Tkhlm+paB7e9twEM/GP6UPx8dFxCW5o7uT/9unfkzg83k1eixwM8TVP+b8sFoup9H2kta3QdEfEBgoFD9VcwxqQCR4ABTXxO5QGe/iKdjdmFPHnhIHqGBtodx+ME+Hrz9+lDKKuq4c4PN+v5AR6mKQWwFogXkRgR8QOmA4sbrLMYuNq6fRHwtTHGWI/xARCRXkBfYFcTn1O5uW8z8nn52ywuG9mT8wZ5zsXcnU181w48cF4iqzPyeePHXXbHUW3ouOPsjDE1InIrsALwBl43xqSIyCNAsjFmMfAa8LaIZAIF1L2hA4wCZotINeAAbjbGHARo7Dlb+LUpJ1ZQWsUdH2yiT9f2zJ2sA8DsdsUpPfk2I595y9M4LTaUxB4d7Y6k2oAY4zof+ZKSkkxycrLdMdRJMsZw8zvrWZl6gE9vGaVvNk6ioLSKCc+vJridL0tuG0WAr8686i5EZJ0xJqnhcj3iptrcog25LN+6n1njE/TN34mEBPnxzCWD2Z53hMc/S7U7jmoDWgCqTeUWljP30xRGRHdm5hi9rKOzGR0fxvWjY3j7592sztBh1+5OC0C1GYfDcOcHm3AYwzMXD8HbS+f5cUZ3nJ1AXHh77vl4M0Xl1XbHUa1IC0C1mTd+3MVPWYd4cFKiDvl0YgG+3jxz8WDySip5dOk2u+OoVqQFoNpEZl4J8z5PY1y/cC4dEXX8ByhbDY7qxE2/781H63L4KlXPEnZXWgCq1dU6DHd8uJkgP2/+eoFO8ewqbh8bT99uHZi9cAuHS6vsjqNagRaAanWvf7+TTdmFPDSlP2Ed/O2Oo5rIz8eLZy4ZzOHSKh5aoqfpuCMtANWqdh4s5ekv0hnXrytTBvewO45qpv49grl9bDyfbtzL8i377I6jWpgWgGo1Dofhno834+fjxePTBuiuHxd10xm9GRgRzIOfbqWwTHcFuRMtANVq3lmzm192FvDgeYl07disSWCVE/H19mLehYMoLKvWE8TcjBaAahU5h8t4cnkao+O7cHFSpN1x1ElK7NGRG34fy4frcvh+u15L2F1oAagWZ4zh3oVbMMAT0wbqrh83cdtZ8cR2CeLeRZspr6q1O45qAVoAqsV9tC6H77Yf5J4JfYkK0RO+3EWArzd/vWAg2QXlPLdSLyPpDrQAVIs6eKSSxz5LZUR0Z2ac2svuOKqFnRIbyuWn9OTV77LYklNkdxx1krQAVIt64rNUyqpqeGLaQLx0rh+3NHtiX7q09+fujzdTXeuwO446CVoAqsX8mHmQhRtyuWFMb+K7drA7jmolHQN8efT8AaTuK+Zf32XZHUedBC0A1SIqqmu5/5Ot9AoN5Naz4uyOo1rZOf27MXFAN55fuZ3dh0rtjqNOkBaAahHzV+1g58FSHp06QK8k5SEemtIfP28v5nyagitdWVD9f1oA6qTtyD/C/FU7mDK4B2P6hNkdR7WRrh0DmDW+D99m5LN8636746gToAWgTooxhvsXbcHf14sHJvWzO45qY1ed1ovE7h15eEkKJRV68RhXowWgTsrC9bn8nFXA7Il9Ce+g0z14Gh/vunme8koqee7L7XbHUc2kBaBO2OHSKh5flsqwnp24bERPu+Momwzt2ZnLR/bk3z/uJGWvnhvgSrQA1Al7cnkaxeXVPHGBjvn3dHef05eQID8e+GQrDoceEHYVWgDqhKzfc5j3k7O5dlQMfbt1tDuOsllwoC/3nduPDXsKWbA22+44qom0AFSz1ToMcz7dSteO/tw+Nt7uOMpJTBsawamxIcz7PI2DRyrtjqOaQAtANdt7v+xha24x953bj/b+PnbHUU5CRHjs/AF1U4Es0+sGuAItANUsh0urePqLdE6JCdFLPKpfiQvvwPWjY1m4PpfkXQV2x1HH0aQCEJEJIpIuIpkiMruR+/1F5H3r/jUiEm0tHy8i60Rki/XvWfUes8p6zo3WV3hLvSjVep5akU5JRQ2PTNVLPKrG3XpWHN2DA5jzaQq1ekDYqR23AETEG3gRmAgkApeJSGKD1a4DDhtj4oDngHnW8oPAZGPMQOBq4O0Gj7vCGDPE+so7ideh2sDmnEIWrN3DNadHk9BNJ3tTjQv08+G+c/uxbV8x7/2yx+446jc05RPASCDTGJNljKkCFgBTG6wzFXjTuv0RMFZExBizwRiz11qeArQTEf+WCK7alsNhePDTFEKD/PnTOD3wq37bpEHdOTU2hKe/SOdwqV5I3lk1pQAigPrjunKsZY2uY4ypAYqA0AbrXAisN8bUHx7whrX750HR/QlO7YPkbDZlF3LfuX3pGOBrdxzl5ESEh6cMoKSihqe/SLc7jjqGNjkILCL9qdstdEO9xVdYu4ZGW18zjvHYmSKSLCLJ+fn5rR9W/UphWRXzPk9jRHRnpg1t2P1KNS6hWwdmnNqLd3/Zw9ZcPUPYGTWlAHKBqHrfR1rLGl1HRHyAYOCQ9X0ksAi4yhiz4+gDjDG51r8lwLvU7Wr6FWPMK8aYJGNMUliYzjRph2e+yKCovJqHp+iBX9U8fxnfh5BAPx5arFNGO6OmFMBaIF5EYkTED5gOLG6wzmLqDvICXAR8bYwxItIJ+AyYbYz54ejKIuIjIl2s277AJGDryb0U1Rq25hbxzprdzDi1F4k99Ixf1TzB7Xy5Z0Jfkncf5pONDf9uVHY7bgFY+/RvBVYAqcAHxpgUEXlERKZYq70GhIpIJjALODpU9FYgDpjTYLinP7BCRDYDG6n7BPGvlnxh6uQ5HIa5i1PoHOjHrLMT7I6jXNRFwyMZHNWJJ5al6ZTRTkZc6WNZUlKSSU5OtjuGx1i0IYe/vL+Jpy4cxCUjoo7/AKWOYWN2Iee/+AMzx8Ry37l63Yi2JiLrjDFJDZfrmcCqUWVVNcxbns7AiGAuGh5pdxzl4oZEdeKSpEhe/34nmXlH7I6jLFoAqlEvrdrB/uIK5kxO1KmeVYu4e0Jf2vl58/ASPSDsLLQA1K/kFpbz8uosJg3qzojoELvjKDfRpb0/fxnXh++2H2Rlqp747wy0ANSvPLk8DYB7dV+tamEzTutFXHh7Hv9sG5U1tXbH8XhaAOp/JO8qYMmmvdwwJpaITu3sjqPcjK+3Fw+c149dh8p488dddsfxeFoA6r8cDsPDS7bRrWMAN57R2+44yk2dkRDOmQlh/OOrTL1wjM20ANR/fbw+hy25RdwzMYFAP73Qi2o9D0xKpLy6lmd0niBbaQEoAI5U1vDUinSGRHVi6mCd70e1rt5h7bn69GgWrM0mZa/OE2QXLQAFwD+/ySS/pJK5OuxTtZHbx8bTOdCPR5Zs02GhNtECUGQXlPHq9zuZNjSCoT072x1HeYjgdr7MGt+HNTsLWL51v91xPJIWgOKJZal4i3DPhL52R1EeZvqIKPp268ATy1KpqNZhoW1NC8DD/Zx1iOVb93PTGb3pFhxgdxzlYXy8vZgzKZGcw+W89v1Ou+N4HC0AD1ZrDfuM6NSOmWNi7Y6jPNTpcV04O7ErL36TyYHiCrvjeBQtAA/2QXI2qfuKmT2xLwG+3nbHUR7s/vP6UVNreOpzHRbalrQAPFRxRTVPr0hnRHRnJg3qbncc5eF6hQZx7agYPl6fw6bsQrvjeAwtAA/1wteZFJRVMWdSf73Mo3IKt54VR5f2/jpbaBvSAvBAOw+W8sYPO7loWCQDI4PtjqMUAO39fbj7nATW7ylk8aa9dsfxCFoAHujxz1Lx8/birgl6mUflXC4aHsmAiI48uTyNsqoau+O4PS0AD/P99oOsTD3ALWfFEd5Bh30q5+LlJcyZ1J99RRW8/G2W3XHcnhaAB6mpdfDI0hSiQtpx7e9i7I6jVKNGxoRw3qDuvLx6B3sLy+2O49a0ADzIe7/sIePAEe6b2E+HfSqndu/Evhjz/y9OpFqHFoCHKCqr5tkvMzglJoQJA7rZHUep3xTZOZCZY2JZvGkv63YX2B3HbWkBeIjnv8qgsLyaOZMTddincgk3/r43XTv68/CSbTgcOiy0NWgBeIDMvCO8/dNupo+Ion8PHfapXEOQvw/3TOjL5pwiFm7ItTuOW9IC8ACPfbaNdr7e3HG2DvtUruX8IREMjurEU5+nUVqpw0JbmhaAm/smPY9V6fncNrbuLEulXImXlzB3ciJ5JZXMX7XD7jhuRwvAjVXXOnhs6TaiQwO55nQd9qlc07CenTl/SA9e+S6L7IIyu+O4FS0AN/afn3ezI7+U+89LxM9H/1Mr13XPxL54i+iw0BbWpHcFEZkgIukikikisxu5319E3rfuXyMi0dby8SKyTkS2WP+eVe8xw63lmSLyd9GhKS3qcGkVz6/czqi4LozrF253HKVOSvfgdtz4+958tmUfa7IO2R3HbRy3AETEG3gRmAgkApeJSGKD1a4DDhtj4oDngHnW8oPAZGPMQOBq4O16j5kPXA/EW18TTuJ1qAaeW5lBSUU1D07SYZ/KPcwcE0uP4AAeWbqNWh0W2iKa8glgJJBpjMkyxlQBC4CpDdaZCrxp3f4IGCsiYozZYIw5Oq1fCtDO+rTQHehojPnZ1M37+hZw/km/GgVAxoES3lmzhytO6UVCtw52x1GqRbTz82b2uf1I2VvMR+uy7Y7jFppSABFA/a2dYy1rdB1jTA1QBIQ2WOdCYL0xptJaP+c4zwmAiMwUkWQRSc7Pz29CXM9mjOHRpdsI8vPmL+P72B1HqRY1eVB3hvfqzN9WpFNSUW13HJfXJkcGRaQ/dbuFbmjuY40xrxhjkowxSWFhYS0fzs18lZrHd9sP8udxfQgJ8rM7jlItSkSYMymRg0eqeOGbTLvjuLymFEAuEFXv+0hrWaPriIgPEAwcsr6PBBYBVxljdtRbP/I4z6maqarGwePLUukdFsSM03rZHUepVjE4qhMXDovkje93sftQqd1xXFpTCmAtEC8iMSLiB0wHFjdYZzF1B3kBLgK+NsYYEekEfAbMNsb8cHRlY8w+oFhETrVG/1wFfHqSr8XjvfnjLnYeLOWBSYn4euuwT+W+7p6QgI+38PhnqXZHcWnHfZew9unfCqwAUoEPjDEpIvKIiEyxVnsNCBWRTGAWcHSo6K1AHDBHRDZaX0fHJN4MvApkAjuA5S31ojzRwSOV/P2r7ZyREMaZCTrsU7m3rh0DuOXMOL7YdoAfMw/aHcdliStdfDkpKckkJyfbHcMp3btwCx8mZ/P5n8cQF97e7jhKtbqK6lrGPfst7f19WHrbKHz0U+8xicg6Y0xSw+W6xdzAtr3FvL92DzNO66Vv/spjBPh6c9+5/UjbX8KCtTos9ERoAbi4o8M+O7bz5c9jddin8iwTB3RjZEwIz36ZQVG5DgttLi0AF7ci5QA/ZR1i1vg+BAf62h1HqTZ1dFjo4bIq/v7VdrvjuBwtABdWWVPLE8tS6dO1PZeP7Gl3HKVsMSAimEuTonjzx11k5R+xO45L0QJwYa9/v4s9BWU8OClRD4Apj3bH2QkE+HrrsNBm0ncNF5VXUsELX29nXL9wRsfrGdLKs4V18Oe2s+L4Ki2P1Rk6ZUxTaQG4qKdXpFNV6+D+8xpOzKqUZ7rmd9H0Cg3k0aXbqKl12B3HJWgBuKCtuUV8uC6Ha06PJqZLkN1xlHIK/j7e3H9uP7bnHeGdNXvsjuMStABcjDGGh5ekEBLox21j4+2Oo5RTGZ/YldN7h/LcygwKy6rsjuP0tABczNLN+1i76zB3nJ1AxwAd9qlUfSLCnMmJFJdX8/xKHRZ6PFoALqSsqoYnlqWS2L0jl46IOv4DlPJAfbt15LKRPXn7591sP1BidxynpgXgQv75zQ72FVXwyNT+eHvpZR6VOpZZ4/sQ6OfNo5+l4krznbU1LQAXsftQKa+szmLa0AiSokPsjqOUUwtt78+fxsazOiOfb9Lz7I7jtLQAXMSjS7fh6y3MntjX7ihKuYSrTosmtksQjy1NpapGh4U2RgvABXyTnsfK1DxuGxtP144BdsdRyiX4+Xjx4OREsg6W8voPO+2O45S0AJxcZU0tjyzZRmyXIK79XYzdcZRyKWcmhDOuX1f+/tV29hWV2x3H6WgBOLnXv6+7zOOcyYn4+eh/LqWaa+7kRGochieWpdkdxenoO4oTO1BcwT++3s64fl05Qy/zqNQJiQoJ5Kbf92bJpr38uEMvH1mfFoAT++uyVGochjmTdL4fpU7GTWf0JrJzOx5anEK1zhP0X1oATmrtrgI+2biXG8bE0jM00O44Srm0AF9v5kxKJOPAEd78cZfdcZyGFoATqnUY5n6aQo/gAG46o7fdcZRyC+MTu3JGQhjPr9xOXkmF3XGcghaAE3r3lz1s21fMfef1I9DPx+44SrkFEWHu5P5U1Th4crkeEAYtAKdTUFrFM1+kc1psKOcN7G53HKXcSkyXIK4fE8PC9bkk7yqwO47ttACczLzlaRypqOGhKf0R0fl+lGppt5wZR4/gAB78NIVah2fPE6QF4ETW7S7g/eRsrhsVQ0K3DnbHUcotBfr58MCkRFL3FfPOmt12x7GVFoCTqKl1cP+irXQPDuB2vdCLUq1q4oBu/C4ulKdXpHPoSKXdcWyjBeAk3vxpN2n7S5g7OZEgfz3wq1RrEhEentKfsqpa5n3uuQeEtQCcwP6iCp79Ip0zEsI4p383u+Mo5RHiwjtw3agYPkjO8dgDwk0qABGZICLpIpIpIrMbud9fRN637l8jItHW8lAR+UZEjojICw0es8p6zo3Wl8fOdfDYZ9uodhge1gO/SrWpP42LJ6JTO+5ftNUjzxA+bgGIiDfwIjARSAQuE5GGcxNcBxw2xsQBzwHzrOUVwIPAncd4+iuMMUOsL4+8asN32/NZunkft5wRR6/QILvjKOVRAv18eHhKf9IPlPDa9543ZXRTPgGMBDKNMVnGmCpgATC1wTpTgTet2x8BY0VEjDGlxpjvqSsC1UBlTS1zPk0hOjSQG34fa3ccpTzSuMSunJ3YledXZpBdUGZ3nDbVlAKIALLrfZ9jLWt0HWNMDVAEhDbhud+wdv88KMfY9yEiM0UkWUSS8/Pzm/CUruOVb7PYebCUR6YOIMDX2+44SnmsuVP64yXCQ4tTPOoawnYeBL7CGDMQGG19zWhsJWPMK8aYJGNMUlhYWJsGbE17DpXxwjeZnDewO2P6uM/rUsoVRXRqx1/G9eGrtDxWpBywO06baUoB5AJR9b6PtJY1uo6I+ADBwKHfelJjTK71bwnwLnW7mjyCMYY5i7fi4yU8qFM9K+UU/vC7aPp178hDi1M4Ulljd5w20ZQCWAvEi0iMiPgB04HFDdZZDFxt3b4I+Nr8xucoEfERkS7WbV9gErC1ueFd1dLN+1iVns+ssxPoFqzX+FXKGWymTCEAAA5GSURBVPh4e/H4tAEcKKnguS8z7I7TJo57xpExpkZEbgVWAN7A68aYFBF5BEg2xiwGXgPeFpFMoIC6kgBARHYBHQE/ETkfOBvYDayw3vy9gZXAv1r0lTmpwrIqHl6SwqDIYK45PdruOEqpeob17MxlI3vyxg87mTY0ggERwXZHalXiSgc8kpKSTHJyst0xTso9H23mo/U5LLl1FIk9OtodRynVQFFZNWOfXUVE50AW3nQ63l6uf26OiKwzxiQ1XK5nArehn3Yc4v3kbK4fHatv/ko5qeBAXx44L5FN2YVuP1mcFkAbqaiu5b5FW+gZEsifdLI3pZza1CE9GB3fhXnL09hbWG53nFajBdBGXvg6k50HS3l82gDa+emYf6WcmYjwxLSBOAw88MlWtz03QAugDaTvL+Glb3dwwbAIRsfrmH+lXEFUSCB3npPA12l5LN601+44rUILoJU5HIbZCzfTsV3dfkWllOu45vRohkR14uEl2ygorbI7TovTAmhl/1mzmw17CnlwUj9CgvzsjqOUagZvL2HehYMoqajmkSUpdsdpcVoArSjncBnzlqcxOr4L5w9pOH2SUsoVJHTrwE1nxPHJxr18k+5ekxZrAbQSYwz3LtwCwBPTBuo8/0q5sFvO7E18eHvuX7jFraaJ0AJoJe+vzea77QeZfW4/okIC7Y6jlDoJ/j7ePHnhIPYVV/CUG11CUgugFewtLOfxz1I5LTaUK0b2tDuOUqoFDO/VmatPi+btn3e7zSUktQBa2NFdPzUOw7wLB+HlBqeRK6Xq3HVOAhGd2nHXR5spr6q1O85J0wJoYR+uy+HbjHxmT+xLz1Dd9aOUOwny9+Gpiwax82ApT61w/V1BWgAtaH9RBY8u3cbImBBmnNrL7jhKqVZweu8uXHN6NG/8sIufdvzmZU+cnhZACzHGcN+iLVTXOnhKd/0o5dbunpBAdGggd320yaVHBWkBtJCF63P5Oi2Pu87pS3SXILvjKKVaUaCfD09fPJjcwnKeWJZqd5wTpgXQAnILy3locQojojvrRV6U8hBJ0SFcPzqWd9fsYXVGvt1xTogWwElyOAx3fLARhzE8c/EQt7h4hFKqaWaN70NceHvu+XgzReXVdsdpNi2Ak/T6Dzv5OauAuZP766gfpTxMgK83z14ymLySSh5dus3uOM2mBXAS0veX8NTn6YxP7MrFSZF2x1FK2WBQZCduPqM3H63LYUXKfrvjNIsWwAmqrKnlz+9vpGM7H/56gc71o5Qnu+2seAZEdGT2x5vJK66wO06TaQGcoOe+3E7qvmKevGAQXdr72x1HKWUjPx8v/m/6UMqra7njw004HK5xBTEtgBPwy84CXl69g8tGRjEusavdcZRSTqB3WHvmTOrPd9sP8voPO+2O0yRaAM1UXFHNrA82EtU5UK/wpZT6H5eNjGJ8Ylee+jydbXuL7Y5zXFoAzWCM4b6FW9hXVMFzlw4hyN/H7khKKSciUncFseBAX/60YAMV1c49YZwWQDN8kJzN0s37mDW+D8N7dbY7jlLKCYUE+fHMxYPZnneEvzr5WcJaAE2UmVfC3MUpnN47lBt/39vuOEopJzamTxjXjYrhzZ9283XaAbvjHJMWQBNUVNdy67sbCPLz4blL9WxfpdTx3XVOAn27deDODzezr6jc7jiN0gJogieWpZK2v4SnLxlM144BdsdRSrmAAF9vXrh8GBXVtdz+3gZqah12R/qVJhWAiEwQkXQRyRSR2Y3c7y8i71v3rxGRaGt5qIh8IyJHROSFBo8ZLiJbrMf8XZz0TKoVKft566fd/HFUDGcmhNsdRynlQuLC2/PEtIGs3XWYZ7/MsDvOrxy3AETEG3gRmAgkApeJSMPxj9cBh40xccBzwDxreQXwIHBnI089H7geiLe+JpzIC2hNOYfLuPujzQyMCObuCX3tjqOUckHnD41g+ogo/rlqB6vS8+yO8z+a8glgJJBpjMkyxlQBC4CpDdaZCrxp3f4IGCsiYowpNcZ8T10R/JeIdAc6GmN+NsYY4C3g/JN5IS2tsqaWW95Zj8Nh+MdlQ/Hz0b1lSqkT89CU/vTt1oFZH2xyquMBTXlXiwCy632fYy1rdB1jTA1QBIQe5zlzjvOcAIjITBFJFpHk/Py2m3P7saWpbMop4m8XD9YLvCilToqzHg9w+j9rjTGvGGOSjDFJYWFhbfIzP9mQy9s/7+aGMbFMGNCtTX6mUsq91T8e8IyTHA9oSgHkAlH1vo+0ljW6joj4AMHAb10tOdd6nt96TltkHCjh3oVbGBkTwl3nJNgdRynlRs4fGsFlI6OYv2qHU0wd3ZQCWAvEi0iMiPgB04HFDdZZDFxt3b4I+Nrat98oY8w+oFhETrVG/1wFfNrs9C2spKKaG99eR5C/Dy9cNhQfb6f/gKSUcjFzJ/dncGQwd3ywicy8I7ZmOe47nLVP/1ZgBZAKfGCMSRGRR0RkirXaa0CoiGQCs4D/DhUVkV3As8A1IpJTbwTRzcCrQCawA1jeMi/pxBhjuOfjzewuKOPFy4cSruP9lVKtIMDXm/lXDsffx4sb3k6mpMK+S0nKb/yh7nSSkpJMcnJyqzz3/FU7mPd5GvdO7MsNOtWDUqqV/bTjEFe+toZx/cKZf8VwvFpxhgERWWeMSWq4XPdxAN+k5fHUijQmDerOzDGxdsdRSnmA03qHcu/EvqxIOcD8b3fYksHjCyAz7wi3v7eBxO4d+dtFg/XSjkqpNnPdqBimDunB01+k821G2w1zP8qjC6CovJqZbyXj7+vFK1cl0c7P2+5ISikPIiI8ecEgErp24LZ317Mjv20PCntsAdQ6DLe/t4Hsw2XMv3I4EZ3a2R1JKeWB2vl586+rkvD19uKPbyZTWFbVZj/bYwtg3udpfJuRz8NTBjAiOsTuOEopDxYVEsjLM4aTe7icm/6znqqatjlT2CMLYMEve3hldRYzTu3F5af0tDuOUkqRFB3CvIsG8lPWIeZ8upW2GKHpcRe1XZ2Rz/2fbGVMnzDmTtaLuiulnMe0oZHsyCvlhW8yiQtvzx9Ht+6oRI8qgLT9xdz8znriw9vz4uV6pq9SyvnMGt+HrINHeHxZKtGhQYxL7NpqP8tj3gEPFFfwhzfWEuTvzRt/GEGHAF+7Iyml1K94eQnPXDyEgRHB3PbeBjbnFLbez2q1Z3YiRypr+MMbaykur+b1a0bQPVhH/CilnFc7P29evTqJ0PZ+XPvvtWQXlLXKz3H7AqipdXDbu+tJP1DCC1cMo3+PYLsjKaXUcYV3CODffxhJda3h6jd+4XBpyw8PdfsC8BIhLrw9D0/pr9f0VUq5lLjw9vzrqiTiw9u3ylUJdTI4pZRyczoZnFJKqf+hBaCUUh5KC0AppTyUFoBSSnkoLQCllPJQWgBKKeWhtACUUspDaQEopZSHcqkTwUQkH9h9gg/vAhxswTgtRXM1j+ZqHs3VPO6aq5cxJqzhQpcqgJMhIsmNnQlnN83VPJqreTRX83haLt0FpJRSHkoLQCmlPJQnFcArdgc4Bs3VPJqreTRX83hULo85BqCUUup/edInAKWUUvVoASillIdy+wIQkb+JSJqIbBaRRSLSqd5994pIpoiki8g5bZzrYhFJERGHiCTVWx4tIuUistH6eskZcln32ba9GuR4SERy622jc+3KYuWZYG2TTBGZbWeW+kRkl4hssbaRbVdSEpHXRSRPRLbWWxYiIl+KyHbr385Oksv23y0RiRKRb0Rkm/X/4p+s5S2/zYwxbv0FnA34WLfnAfOs24nAJsAfiAF2AN5tmKsfkACsApLqLY8Gttq4vY6Vy9bt1SDjQ8Cddv9uWVm8rW0RC/hZ2yjR7lxWtl1AFyfIMQYYVv/3GngKmG3dnn30/0snyGX77xbQHRhm3e4AZFj//7X4NnP7TwDGmC+MMTXWtz8DkdbtqcACY0ylMWYnkAmMbMNcqcaY9Lb6eU31G7ls3V5ObCSQaYzJMsZUAQuo21bKYoxZDRQ0WDwVeNO6/SZwfpuG4pi5bGeM2WeMWW/dLgFSgQhaYZu5fQE0cC2w3LodAWTXuy/HWuYMYkRkg4h8KyKj7Q5jcbbtdau1W+91O3Yf1ONs26U+A3whIutEZKbdYRroaozZZ93eD3S1M0wDzvK7hYhEA0OBNbTCNvM52SdwBiKyEujWyF33G2M+tda5H6gB3nGmXI3YB/Q0xhwSkeHAJyLS3xhTbHOuNvVbGYH5wKPUvcE9CjxDXbmr/zXKGJMrIuHAlyKSZv3V61SMMUZEnGU8utP8bolIe+Bj4M/GmGIR+e99LbXN3KIAjDHjfut+EbkGmASMNdYONCAXiKq3WqS1rM1yHeMxlUCldXudiOwA+gAtdhDvRHLRBturvqZmFJF/AUtbK0cTtOl2aQ5jTK71b56ILKJud5WzFMABEelujNknIt2BPLsDARhjDhy9befvloj4Uvfm/44xZqG1uMW3mdvvAhKRCcDdwBRjTFm9uxYD00XEX0RigHjgFzsy1iciYSLibd2OpS5Xlr2pACfaXtYv/1HTgK3HWrcNrAXiRSRGRPyA6dRtK1uJSJCIdDh6m7rBEHZup4YWA1dbt68GnOWTp+2/W1L3p/5rQKox5tl6d7X8NrPzaHcbHVHPpG4f7Ubr66V6991P3QiOdGBiG+eaRt3+4krgALDCWn4hkGJlXQ9MdoZcdm+vBhnfBrYAm63/Kbrb/Dt2LnUjNXZQtxvNtiz1MsVSNyJpk/X7ZFsu4D3qdm1WW79b1wGhwFfAdmAlEOIkuWz/3QJGUbcLanO9961zW2Ob6VQQSinlodx+F5BSSqnGaQEopZSH0gJQSikPpQWglFIeSgtAKaU8lBaAUkp5KC0ApZTyUP8PC/5MlgLyipgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TK7xsbpxeeBI"
      },
      "source": [
        "noise_input = []\n",
        "output_fake_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Y3ee9efaxp"
      },
      "source": [
        "s = np.random.normal(0,1, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dpd1DskfbOF"
      },
      "source": [
        "for i in range(1000):\n",
        "  noice = torch.zeros(1,100,device = device).uniform_(0,1)\n",
        "  gen_rep = generator(noice)\n",
        "  noise_input.append(noice)\n",
        "  output_fake_data.append(gen_rep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G59lClEgiWmD"
      },
      "source": [
        "noise_input = np.array(noise_input)\n",
        "output_fake_data = np.array(output_fake_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "armmi5s4fe3E",
        "outputId": "e43c1792-1e98-48b7-c6e3-455c60588e19"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "M = np.random.rand(944, 1683)\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(M)\n",
        "\n",
        "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
        "t = reduced.transpose()\n",
        "\n",
        "plt.scatter(t[0], t[1])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ]
}